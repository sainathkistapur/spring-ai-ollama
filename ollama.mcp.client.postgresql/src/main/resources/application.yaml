spring:
  application:
    name: springai.ollama.basic.chat

  threads:
    virtual:
      enabled: true

  ai:
    ollama:
      chat:
        options:
          model: llama3.2:3b
      init:
        pull-model-strategy: WHEN_MISSING
        embedding:
          include: false
    mcp:
      client:
        toolcallback:
          enabled: true
        stdio:
          servers-configuration: classpath:mcp-servers.json

  docker:
    compose:
      lifecycle-management: start-and-stop
      stop:
        command: down
        timeout: 1m
